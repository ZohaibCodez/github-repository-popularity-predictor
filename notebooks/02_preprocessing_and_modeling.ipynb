{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507d6c03",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014afbf5",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc56845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/repositories.csv')\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e4d2e",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b673cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing Values Before Preprocessing:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Language: Fill with 'Unknown'\n",
    "if 'Language' in df.columns:\n",
    "    df['Language'] = df['Language'].fillna('Unknown')\n",
    "\n",
    "# License: Fill with 'No License'\n",
    "if 'License' in df.columns:\n",
    "    df['License'] = df['License'].fillna('No License')\n",
    "\n",
    "# For other columns, fill numeric with median, categorical with mode\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"\\nMissing Values After Preprocessing:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468936d9",
   "metadata": {},
   "source": [
    "### 3.2 Remove Multicollinearity\n",
    "\n",
    "**Issue:** Watchers ‚âà Stars (correlation ‚âà 0.99) causes perfect multicollinearity.  \n",
    "**Solution:** Drop Watchers column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969709b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Watchers column (redundant with Stars)\n",
    "if 'Watchers' in df.columns:\n",
    "    print(\"Dropping 'Watchers' column (multicollinearity with Stars)\")\n",
    "    df = df.drop('Watchers', axis=1)\n",
    "    \n",
    "print(f\"\\nDataset shape after dropping Watchers: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8554e01",
   "metadata": {},
   "source": [
    "### 3.3 Feature Selection for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for modeling\n",
    "# Target: Stars\n",
    "# Features: Numerical + Top categorical features\n",
    "\n",
    "# Define feature columns\n",
    "numerical_features = ['Forks', 'Open Issues', 'Size']\n",
    "categorical_features = ['Language', 'Has Wiki', 'Has Issues', 'Has Projects']\n",
    "\n",
    "# Keep only relevant columns\n",
    "columns_to_keep = ['Stars'] + numerical_features + categorical_features\n",
    "\n",
    "# Filter columns that exist in dataframe\n",
    "columns_to_keep = [col for col in columns_to_keep if col in df.columns]\n",
    "\n",
    "df_model = df[columns_to_keep].copy()\n",
    "\n",
    "print(f\"Features for modeling: {df_model.columns.tolist()}\")\n",
    "print(f\"Dataset shape: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc3001",
   "metadata": {},
   "source": [
    "### 3.4 Handle Extreme Skewness\n",
    "\n",
    "Apply log transformation to highly skewed numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a694697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform skewed numerical features (add 1 to avoid log(0))\n",
    "for col in numerical_features:\n",
    "    if col in df_model.columns:\n",
    "        df_model[f'{col}_log'] = np.log1p(df_model[col])\n",
    "        print(f\"Created log-transformed feature: {col}_log\")\n",
    "\n",
    "# Also transform target variable\n",
    "df_model['Stars_log'] = np.log1p(df_model['Stars'])\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8083fa3",
   "metadata": {},
   "source": [
    "### 3.5 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa787ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in df_model.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_model[f'{col}_encoded'] = le.fit_transform(df_model[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(f\"\\nTotal label encoders saved: {len(label_encoders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65d9f2",
   "metadata": {},
   "source": [
    "### 3.6 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final features for modeling\n",
    "feature_columns = [col for col in df_model.columns if ('_log' in col or '_encoded' in col) and col != 'Stars_log']\n",
    "\n",
    "X = df_model[feature_columns]\n",
    "y = df_model['Stars_log']  # Use log-transformed target\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8638ea",
   "metadata": {},
   "source": [
    "### 3.7 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Train/Test split: {X_train.shape[0]/len(X)*100:.1f}% / {X_test.shape[0]/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e4003",
   "metadata": {},
   "source": [
    "### 3.8 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Feature scaling completed\")\n",
    "print(f\"Scaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bbc08",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "Train multiple models and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2698c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(f\"Total models to train: {len(models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "    \n",
    "    print(f\"Training R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"Testing R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"Testing RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"Testing MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a531976",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a82f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train R¬≤': [results[m]['train_r2'] for m in results],\n",
    "    'Test R¬≤': [results[m]['test_r2'] for m in results],\n",
    "    'Test RMSE': [results[m]['test_rmse'] for m in results],\n",
    "    'Test MAE': [results[m]['test_mae'] for m in results]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Test R¬≤', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "comparison_df_sorted = comparison_df.sort_values('Test R¬≤')\n",
    "axes[0].barh(comparison_df_sorted['Model'], comparison_df_sorted['Test R¬≤'], color='steelblue')\n",
    "axes[0].set_xlabel('R¬≤ Score')\n",
    "axes[0].set_title('Model Performance Comparison (R¬≤ Score)')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "comparison_df_sorted_rmse = comparison_df.sort_values('Test RMSE', ascending=False)\n",
    "axes[1].barh(comparison_df_sorted_rmse['Model'], comparison_df_sorted_rmse['Test RMSE'], color='coral')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('Model Performance Comparison (RMSE - Lower is Better)')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f178b",
   "metadata": {},
   "source": [
    "## 6. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on Test R¬≤\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  ‚Ä¢ Training R¬≤: {results[best_model_name]['train_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Testing R¬≤: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Testing RMSE: {results[best_model_name]['test_rmse']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Testing MAE: {results[best_model_name]['test_mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dda8cd",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfadb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importance if model supports it\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='forestgreen')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\nNote: {best_model_name} does not support feature importance extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403637a1",
   "metadata": {},
   "source": [
    "## 8. Prediction vs Actual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert back from log scale\n",
    "y_test_actual = np.expm1(y_test)\n",
    "y_pred_actual = np.expm1(y_pred_best)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_actual, y_pred_actual, alpha=0.3, s=10)\n",
    "plt.plot([y_test_actual.min(), y_test_actual.max()], \n",
    "         [y_test_actual.min(), y_test_actual.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Stars')\n",
    "plt.ylabel('Predicted Stars')\n",
    "plt.title(f'Predicted vs Actual Stars - {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396f14d",
   "metadata": {},
   "source": [
    "## 9. Save Models and Preprocessors\n",
    "\n",
    "Save all necessary objects for deployment in Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, f'{models_dir}/best_model.pkl')\n",
    "print(f\"‚úÖ Saved best model: {best_model_name}\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, f'{models_dir}/scaler.pkl')\n",
    "print(f\"‚úÖ Saved scaler\")\n",
    "\n",
    "# Save label encoders\n",
    "joblib.dump(label_encoders, f'{models_dir}/label_encoders.pkl')\n",
    "print(f\"‚úÖ Saved label encoders\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_columns, f'{models_dir}/feature_columns.pkl')\n",
    "print(f\"‚úÖ Saved feature columns\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'test_r2': results[best_model_name]['test_r2'],\n",
    "    'test_rmse': results[best_model_name]['test_rmse'],\n",
    "    'test_mae': results[best_model_name]['test_mae'],\n",
    "    'training_samples': X_train.shape[0],\n",
    "    'testing_samples': X_test.shape[0],\n",
    "    'features': feature_columns\n",
    "}\n",
    "joblib.dump(metadata, f'{models_dir}/model_metadata.pkl')\n",
    "print(f\"‚úÖ Saved model metadata\")\n",
    "\n",
    "# Save all models for comparison\n",
    "for name, result in results.items():\n",
    "    model_filename = name.lower().replace(' ', '_')\n",
    "    joblib.dump(result['model'], f'{models_dir}/{model_filename}.pkl')\n",
    "    print(f\"‚úÖ Saved {name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"All models and preprocessors saved to '{models_dir}/'\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221bf26",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Data Preprocessing Steps Completed:\n",
    "1. ‚úÖ Handled missing values (Language, License)\n",
    "2. ‚úÖ Removed multicollinearity (dropped Watchers)\n",
    "3. ‚úÖ Log-transformed skewed features\n",
    "4. ‚úÖ Encoded categorical variables\n",
    "5. ‚úÖ Scaled numerical features\n",
    "6. ‚úÖ Split into train/test sets (80/20)\n",
    "\n",
    "### Models Trained:\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. Random Forest\n",
    "5. Gradient Boosting\n",
    "6. XGBoost\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy best model in Streamlit application\n",
    "- Enable runtime predictions based on user input\n",
    "- Visualize model performance interactively"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
